### ğŸš€ **README.md**

# ğŸŒ Scrapy-HTML

ğŸ” **Scrapy-HTML** Ã© um pacote Python simples e eficiente para scraping de conteÃºdo HTML de qualquer pÃ¡gina web. Agora com **parÃ¢metros avanÃ§ados** para filtragem por tag, mÃºltiplas tags, classe CSS e atributos personalizados, tornando a raspagem muito mais flexÃ­vel e poderosa. Ele utiliza as bibliotecas **BeautifulSoup4** e **Requests** para realizar a raspagem e retornar o HTML de forma estruturada e legÃ­vel.

---

## âœ¨ **CaracterÃ­sticas Principais**

- ğŸŒ Faz scraping de qualquer pÃ¡gina web com uma URL vÃ¡lida.
- ğŸ” **Filtragem avanÃ§ada** por:
  - Tag Ãºnica (`tag`)
  - MÃºltiplas tags (`tags`)
  - Classe CSS (`class_`)
  - ID (`id_`)
  - Atributos personalizados (`attrs`)
  - Headers HTTP personalizados (`headers`)
- ğŸŒ **Suporte a Proxies**:
  - ConfiguraÃ§Ã£o de proxies HTTP/HTTPS
  - Timeout personalizado para requisiÃ§Ãµes via proxy
- âš¡ **MÃºltiplos parsers HTML**:
  - `html.parser` (padrÃ£o): Parser nativo do Python
  - `lxml`: Parser rÃ¡pido baseado em C
  - `html5lib`: Parser mais leniente e compatÃ­vel com HTML5
- âš¡ Retorna o HTML formatado e legÃ­vel usando `BeautifulSoup`.
- ğŸ”’ Tratamento de erros robusto para URLs invÃ¡lidas ou problemas de rede.
- ğŸ’¡ Leve e fÃ¡cil de usar, com dependÃªncias mÃ­nimas.
- ğŸ§¹ **ValidaÃ§Ã£o e Limpeza de Dados**:
  - Limpeza de texto e HTML
  - ValidaÃ§Ã£o de URLs
  - SanitizaÃ§Ã£o de atributos HTML
  - ExtraÃ§Ã£o de dados estruturados (meta tags, JSON-LD, Open Graph, Twitter Cards)

---

## âš¡ **InstalaÃ§Ã£o**

Instale o pacote diretamente do **PyPI**:

```bash
pip install scrapy_html
```

### ğŸ“¦ **Parsers Opcionais**

Para usar parsers alternativos, instale as dependÃªncias opcionais:

```bash
# Para usar o parser lxml (mais rÃ¡pido)
pip install scrapy_html[lxml]

# Para usar o parser html5lib (mais leniente)
pip install scrapy_html[html5lib]

# Para usar todos os parsers
pip install scrapy_html[lxml,html5lib]
```

---

## ğŸ’» **Como Usar**

### ğŸ”¥ **Exemplo bÃ¡sico de uso:**
```python
from scrapy_html.scraper import get_html_content

# ğŸŒ URL da pÃ¡gina
url = "https://www.example.com"

# ğŸ”„ Obtendo o conteÃºdo HTML completo
dados = get_html_content(url)
print(dados)
```

### ğŸ¯ **Filtragem AvanÃ§ada:**

#### ğŸ” **Filtrar por tag Ãºnica:**
```python
dados = get_html_content(url, tag="p")
print(dados)  # Exibe todas as tags <p>
```

#### ğŸ· **Filtrar por mÃºltiplas tags:**
```python
dados = get_html_content(url, tags="div,span")
print(dados)  # Exibe todas as tags <div> e <span>
```

#### ğŸ¨ **Filtrar por classe CSS:**
```python
dados = get_html_content(url, tag="a", class_="link-destaque")
print(dados)
```

#### ğŸ†” **Filtrar por ID:**
```python
dados = get_html_content(url, tag="section", id_="conteudo-principal")
print(dados)
```

#### ğŸ›  **Filtrar por atributos personalizados:**
```python
dados = get_html_content(url, tag="img", attrs={"alt": "Imagem principal"})
print(dados)
```

#### ğŸ”’ **Usar headers personalizados:**
```python
headers = {
    "User-Agent": "Mozilla/5.0",
    "Accept": "text/html",
    "Accept-Language": "pt-BR"
}
dados = get_html_content(url, headers=headers)
print(dados)
```

#### ğŸ” **Usar diferentes parsers HTML:**
```python
# Usando o parser padrÃ£o (html.parser)
dados = get_html_content(url)
print(dados)

# Usando o parser lxml (mais rÃ¡pido)
dados = get_html_content(url, parser="lxml")
print(dados)

# Usando o parser html5lib (mais leniente)
dados = get_html_content(url, parser="html5lib")
print(dados)
```

#### ğŸŒ **Usar proxies:**
```python
proxies = {
    "http": "http://proxy1:8080",
    "https": "https://proxy2:8080"
}
dados = get_html_content(
    url,
    proxies=proxies,
    proxy_timeout=30  # timeout em segundos
)
print(dados)
```

---

## ğŸ›  **Requisitos**

- Python >= 3.6
- [beautifulsoup4](https://pypi.org/project/beautifulsoup4/)
- [requests](https://pypi.org/project/requests/)

As dependÃªncias sÃ£o instaladas automaticamente com:

```bash
pip install scrapy_html
```

---

## ğŸ§ª **Testes**

Este projeto inclui testes usando **pytest**. Para rodar os testes localmente:

```bash
pip install pytest
pytest tests/
```

---

## ğŸ¨ **Recursos Futuros**

- [x] ğŸ” ParÃ¢metros avanÃ§ados para scraping filtrado.
- [x] ğŸŒ Suporte a diferentes parsers (`lxml`, `html5lib`).
- [x] ğŸŒ Suporte a proxies HTTP/HTTPS.
- [ ] ğŸ”„ Scraping assÃ­ncrono para maior desempenho.
- [ ] âš¡ Download de recursos estÃ¡ticos (imagens, CSS, JS).
- [ ] ğŸ§ª Testes automatizados avanÃ§ados com `requests-mock`.

---

## ğŸ— **Estrutura do Projeto**

```
scrapy_html/
â”‚
â”œâ”€â”€ scrapy_html/             # ğŸ“¦ CÃ³digo principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ scraper.py           # âš¡ FunÃ§Ã£o principal: get_html_content()
â”‚
â”œâ”€â”€ tests/                   # ğŸ§ª Testes automatizados
â”‚   â””â”€â”€ test_scraper.py
â”‚
â”œâ”€â”€ setup.py                 # âš™ï¸ ConfiguraÃ§Ã£o para PyPI
â”œâ”€â”€ pyproject.toml           # ğŸ“¦ ConfiguraÃ§Ã£o moderna
â”œâ”€â”€ README.md                # ğŸ“š DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ LICENSE                  # ğŸ“œ LicenÃ§a MIT
â””â”€â”€ MANIFEST.in              # ğŸ“‹ InclusÃ£o de arquivos extras
```

---

## ğŸ”§ **Contribuindo**

ContribuiÃ§Ãµes sÃ£o bem-vindas! ğŸš€  
Para contribuir, siga estas etapas:

1. **Fork** este repositÃ³rio.
2. Crie uma nova branch:
   ```bash
   git checkout -b minha-nova-funcionalidade
   ```
3. FaÃ§a suas alteraÃ§Ãµes e commit:
   ```bash
   git commit -m "âœ¨ Adicionando nova funcionalidade incrÃ­vel"
   ```
4. Envie para o branch:
   ```bash
   git push origin minha-nova-funcionalidade
   ```
5. **Abra um Pull Request**. ğŸ’¡

---

## ğŸ‘¨â€ğŸ’» **Autor**

Desenvolvido por **[Roberto Lima](https://github.com/robertolima-dev)** ğŸš€âœ¨

---

## ğŸ’¬ **Contato**

- ğŸ“§ **Email**: robertolima.izphera@gmail.com
- ğŸ’¼ **LinkedIn**: [Roberto Lima](https://www.linkedin.com/in/roberto-lima-01/)
- ğŸ’¼ **Website**: [Roberto Lima](https://robertolima-developer.vercel.app/)
- ğŸ’¼ **Gravatar**: [Roberto Lima](https://gravatar.com/deliciouslyautomaticf57dc92af0)


---

## ğŸ“„ LicenÃ§a
MIT License

### ğŸ§¹ **ValidaÃ§Ã£o e Limpeza de Dados:**

```python
from scrapy_html import DataValidator

validator = DataValidator()

# Limpeza de texto
texto_limpo = validator.clean_text("  Hello   World  ")
print(texto_limpo)  # "Hello World"

# Limpeza de HTML
html_limpo = validator.clean_html("<script>alert('test');</script><p>Hello</p>")
print(html_limpo)  # "<p>Hello</p>"

# ValidaÃ§Ã£o de URL
url_valida = validator.validate_url("http://example.com")
print(url_valida)  # True

# SanitizaÃ§Ã£o de atributos
from bs4 import BeautifulSoup
html = '<a href="http://example.com" onclick="alert(1)">Link</a>'
soup = BeautifulSoup(html, 'html.parser')
tag = soup.find('a')
tag_sanitizada = validator.sanitize_attributes(tag)
print(tag_sanitizada)  # <a href="http://example.com">Link</a>

# ExtraÃ§Ã£o de dados estruturados
dados = validator.extract_structured_data("""
    <meta name="description" content="Test">
    <meta property="og:title" content="Test OG">
    <script type="application/ld+json">
        {"@type": "WebPage"}
    </script>
""")
print(dados)  # {'meta': {'description': 'Test'}, 'open_graph': {'title': 'Test OG'}, ...}
```